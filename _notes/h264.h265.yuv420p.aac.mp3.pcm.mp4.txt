数据源.player--surface--surfaceTexture--(Glsurfaceview旋转90度?.opengles.egl: Display.Surface.Context)--textureView：每一层都有回调函数设置,回调传递上层运行所需的所有变量和数据。
(视频数据源可获取sps,pps.GOP.宽.高.帧率, I.B.P帧 IDR帧即强制刷新帧，强制刷新宽)
RAWrgb--yuv420.yuv420sp(x264编码-->h264.h265裸流解码-->yuv420p.yuv420sp--)--rgba--jpg.png
YUV Planar(p)：Y\U\V数据是分开存放的，在三个不同的平面（数组）
YUV Semi-Planar(sp)：Y是单独一个平面（数组），而U、V是交叉存放的，在另一个平面（数组）
I420: YYYYYYYY UU VV      =>YUV420P
YV12: YYYYYYYY VV UU     =>YUV420P
NV12: YYYYYYYY UVUV     =>YUV420SP
NV21: YYYYYYYY VUVU     =>YUV420SP
https://www.polarxiong.com/archives/Android-MediaCodec%E8%A7%86%E9%A2%91%E6%96%87%E4%BB%B6%E7%A1%AC%E4%BB%B6%E8%A7%A3%E7%A0%81-%E9%AB%98%E6%95%88%E7%8E%87%E5%BE%97%E5%88%B0YUV%E6%A0%BC%E5%BC%8F%E5%B8%A7-%E5%BF%AB%E9%80%9F%E4%BF%9D%E5%AD%98JPEG%E5%9B%BE%E7%89%87-%E4%B8%8D%E4%BD%BF%E7%94%A8OpenGL.html
libyuv是Google开源的yuv图像处理库，实现对各种yuv数据(byte[]数组)之间的转换，包括数据转换，裁剪，缩放，旋转。

gop(group of picture图像序列)
GOP结构一般两个数字，如M=3，N=12。M值表示I帧或者P帧之间的帧数目，N值表示一个GOP的帧数。上面的M=3，N=12，GOP结构为：IBBPBBPBBPBBI。
一个gop包的第一帧一定是IDR帧。一个GOP时长的计算公式：duration=N/FPS。其中，N表示GOP帧数，FPS表示帧率。比如，N=72，FPS=24，那么GOP时长d=N/FPS=3s。
关于GOP我看到2种说法，一种说法如上，1个GOP里面只有一个I帧，第二种说法是1个GOP里面可以有好几个I帧，所以我就糊涂了。
第一种说法是针对Mpeg2的，这里面一个GOP只有且只有一个在组头的I帧；第二种说法是针对h264的新特。
结论：
每个GOP一定是以一个I帧开始的，但是却不一定指代的是两个I帧之间的距离。因为一个GOP内可能包含几个I帧，只有第一个I帧（也就是第一帧）才是关键帧。
https://blog.csdn.net/qq_32245927/article/details/80029949
一个gop包中有sps，pps。sps中包含了此序列图片的宽.高.帧率等信息。一个序列GOP的第一个图像帧叫做 IDR帧即强制刷新帧，强制刷新宽.高.频率等信息(属于特殊的Ｉ帧)。

IDR  = instantaneous decoding refresh 即时解码刷新。
目的：【为了解码的重同步】
当解码器解码到 IDR 图像时【相当于向解码器发送了一个清理reference buffer的信号】，立即将参考帧队列清空【驱动器参数块DPB(decord picture buff 参考帧列表)→导致PPS、SPS参数更新】
，将已解码的数据全部输出或抛弃，重新查找参数集，开始一个新的序列。
如果前一个序列出现重大错误，在这里可以获得重新同步的机会。
//为了取得更高的图像质量，在一个视频文件中有好多个IDR帧
IDR frame：I和IDR帧都使用帧内预测，在编码解码中为了方便，首个I帧要和其他I帧区别开，把第一个I帧叫IDR，这样方便控制编码和解码流程，所以IDR帧一定是I帧，但I帧不一定是IDR帧；
IDR帧的作用是立刻刷新,使错误不致传播,从IDR帧开始算新的序列开始编码。I帧有被跨帧参考的可能,IDR不会。

i帧 i frame, intra picture 非参考帧/独立帧  帧内编码帧👈帧内压缩
	通常是GOP的第一个帧(即IDR)，也可能在一个序列中间插入I帧
	一般每秒至少一个I帧
	I帧丢失→随后的P帧也就无法解析	
使用 ffmpeg 获取 I 帧：
指令为：ffmpeg -i wkll.mp4 -ss 00:00:00 -t 1 -r 1 -q:v 2 -f image2 pic-%03d.jpeg

PTS(Presentation Time Stamp)和DTS(Decode Time Stamp): PTS就说这个帧什么时候会放在显示器上;DTS就是说这个帧什么时候被放在编码器去解。
那么如果全是I帧和P帧，PTS和DTS都是单调递增的，那么如果我们有B帧，会出现什么情况?因为大家都知道，对于B帧来讲，它会引用前面的帧和后面的帧。
我们看这个例子，就是当B帧进来的时候，因为它要引用后面的P帧，也要引用前面的I帧，可以看到DTS的顺序，一三四二，然后PTS顺序，一二三四。
B帧它会根据它编码时候的特性，它会自动的把它的DTS时间戳往后挪，把它引用的帧先放到前面去，等它引用的帧解完了，数据解完了之后，才会把B帧去解，
否则的话，我先把B帧放进去，它引用后面的P帧，P帧的数据还没有，B帧解不出来。
所以说这点上给大家讲一下，B帧，P帧，I帧它们整个放在一个视频流里面，它的解码顺序和编码顺序，当然后续的话，我们可能会根据这个有一些开放性的思考。

H264流 gop包Header(NALU Unit)：sps(Sequence Paramater Set) pps(Picture Paramater Set) 视频宽.高。 码率.帧率.分辨率  I关键帧.B.P帧 IDR帧即强制刷新帧，强制刷新宽
https://jingyan.baidu.com/article/3c48dd34c06ac0e10ae35841.html
https://blog.csdn.net/DaveBobo/article/details/75041348
H.264原始码流（又称为“裸流”）是由一个一个的NALU组成的。
其中每个NALU之间通过startcode（起始码）进行分隔，起始码分成两种：0x000001（3Byte）或者0x00000001（4Byte）。如果NALU对应的Slice为一帧的开始就用0x00000001，否则就用0x000001。
H.264码流解析的步骤就是首先从码流中搜索0x000001和0x00000001，分离出NALU；然后再分析NALU的各个字段。本文的程序即实现了上述的两个步骤。
https://blog.csdn.net/leixiaohua1020/article/details/50534369

camera YUV 数据格式知识：
https://blog.csdn.net/whyqcwy/article/details/101546043

ffmpeg解码H264
android下使用ffmpeg解码函数播放H264裸流
https://blog.csdn.net/starhosea/article/details/73128120
ffmpeg解码H264，并转换成RGB显示到surface上

雷神 csdn ffmpeg
https://www.jianshu.com/p/558855026e95
https://blog.csdn.net/leixiaohua1020/article/details/15811977
http://blog.csdn.net/leixiaohua1020
https://www.zhihu.com/people/leixiaohua1020
http://jhdr.xhby.net/content/201608/03/c213550.html

MediaCodec：
https://blog.csdn.net/u012521570/article/details/78783294

android 如何保存opengl美颜过的视频
https://blog.csdn.net/u010029439/article/details/89494475

h265.HEVC：
https://blog.csdn.net/NB_vol_1/article/details/51143186

数据源、surface、surfaceTexture(onFrameAvailable(...))、GlSurfaceView(onDrawFrame(GL10 gl))、TextureView各层的回调
数据源-->Surface-->SurfaceTexture-->GlSurfaceView(textureID)、TextureView
1.LocalVideo(ijkPlayer.setDataSource())
2.UsbDevice外设(MediaCodec)：
3.Camera



