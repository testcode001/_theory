音视频录制Camera MediaCodec.MediaMuxer.AudioRecoder.MediaRecoder OpenGLES.Vulcan ，音视频编辑ffmpeg.VLC.vitamo.OpenGLES，音视频播放ijkplayer.MediaPlayer.AudioTrack.ffmpeg 图像编辑
RTP实时音视频

数据源.player--surface--surfaceTexture--(Glsurfaceview旋转90度?.opengles.egl: Display.Surface.Context)--textureView：每一层都有回调函数设置,回调传递给下一级(即上层)运行所需的所有变量和数据。
(视频数据源可获取SPS,PPS.GOP.宽.高.帧率, I.B.P帧 IDR帧即强制刷新帧，强制刷新宽)
音视频：采集-->x264编码-->网络传输或保存成文件-->解码-->展示
其中音视频解码-->展示又分为一下几种：
1.USB Device外设：MedicCodec硬解码
2.camera sensor：系统camera类解码
3.本地文件、网络文件：ijkplayer解码
RAWrgb--yuv420(x264编码-->h264.h265裸流-->解码成yuv420p.yuv420sp--)--rgba--jpg.png
yuv420分为yuv420p和yuv420sp
YUV Planar(p)：Y\U\V数据是分开存放的，在三个不同的平面（数组）
YUV Semi-Planar(sp)：Y是单独一个平面（数组），而U、V是交叉存放的，在另一个平面（数组）
I420: YYYYYYYY UU VV      =>YUV420P
YV12: YYYYYYYY VV UU     =>YUV420P
NV12: YYYYYYYY UVUV     =>YUV420SP
NV21: YYYYYYYY VUVU     =>YUV420SP
https://www.polarxiong.com/archives/Android-MediaCodec%E8%A7%86%E9%A2%91%E6%96%87%E4%BB%B6%E7%A1%AC%E4%BB%B6%E8%A7%A3%E7%A0%81-%E9%AB%98%E6%95%88%E7%8E%87%E5%BE%97%E5%88%B0YUV%E6%A0%BC%E5%BC%8F%E5%B8%A7-%E5%BF%AB%E9%80%9F%E4%BF%9D%E5%AD%98JPEG%E5%9B%BE%E7%89%87-%E4%B8%8D%E4%BD%BF%E7%94%A8OpenGL.html
libyuv是Google开源的yuv图像处理库，实现对各种yuv数据(byte[]数组)之间的转换，包括数据转换，裁剪，缩放，旋转。

usb串口通信：自定义数据包分隔符MO+header区(数据类型ContentType.数据大小ContentLength)+数据区(音频.视频.json数据)。 header区版本号，header区和数据区的crc校验
数据类型ContentType: 音频，视频，下载文件，json应答
全局唯一、递增的请求.响应msgId + HashMap，以区分不同的回调函数,保证应答不错乱
usb通信都是串行通信？ 下载的应答的uri为啥要加上偏移-批量发送下载请求
视频数据区字节转model、时间戳是所在视频文件的时间戳吗？

音视频相关技术h264.h265组成结构：
H.264原始码流（又称为“裸流”）是由一个一个的NALU组成的。
其中每个NALU之间通过startcode（起始码）进行分隔，起始码分成两种：0x000001（3Byte）或者0x00000001（4Byte）。如果NALU对应的Slice为一帧的开始就用0x00000001，否则就用0x000001。
H.264码流解析的步骤就是首先从码流中搜索0x000001和0x00000001，分离出NALU；然后再分析NALU的各个字段。本文的程序即实现了上述的两个步骤。
如果NALU的RBSP(Raw Byte Sequence Payload)数据内部是有可能含有0x000001和0x00000001这种字节序列的，为了防止解析错误，所以在RBSP数据流里面碰到0x 00 00 00 01的0x01前面就会加上0x03，解码时将NALU的EBSP中的0x03去掉成为RBSP，称为脱壳操作。
https://blog.csdn.net/leixiaohua1020/article/details/50534369
我们发现视频流数据就是由一系列SPS、PPS、I、P、B帧序列组成，这些数据都是通过NALU进行承载的；
我们看到了NALU不仅仅可以承载SPS、PPS、SEI等非VCL数据，也可以传输I、B、P帧的切片VCL数据。
我们同时看到了NALU的Data部分，如果是VCL数据，则就是slice header+silce data这种结构，其中对VCL的SODB做了bit填充的字节对齐处理；
这里由于没有数据分割机制，所以一个NALU承载一个片，同时一个片就是一个视频帧；
5.至于NALU的非VCL数据SPS、PPS、SEI各个字段的含义具体解析放到下篇文章，这个信息对于解码器进行播放视频很重要，很多播放问题都是这个数据有问题导致的；
https://blog.csdn.net/sunshineywz/article/details/108089528

gop(group of picture图像序列)
H264流 gop包Header(NALU Unit)：SPS(Sequence Paramater Set) PPS(Picture Paramater Set) 视频宽.高。 码率.帧率.分辨率  I关键帧.B.P帧 IDR帧即强制刷新帧，强制刷新宽
一个gop包中有sps，pps。sps中包含了此序列图片的宽.高.帧率等信息。一个序列GOP的第一个图像帧叫做 IDR帧即强制刷新帧，强制刷新宽.高.频率等信息(属于特殊的Ｉ帧)。
https://jingyan.baidu.com/article/3c48dd34c06ac0e10ae35841.html
https://blog.csdn.net/DaveBobo/article/details/75041348
GOP结构一般两个数字，如M=3，N=12。M值表示I帧或者P帧之间的帧数目，N值表示一个GOP的帧数。上面的M=3，N=12，GOP结构为：IBBPBBPBBPBBI。
一个gop包的第一帧一定是IDR帧。一个GOP时长的计算公式：duration=N/FPS。其中，N表示GOP帧数，FPS表示帧率。比如，N=72，FPS=24，那么GOP时长d=N/FPS=3s。
关于GOP我看到2种说法，一种说法如上，1个GOP里面只有一个I帧，第二种说法是1个GOP里面可以有好几个I帧，所以我就糊涂了。
第一种说法是针对Mpeg2的，这里面一个GOP只有且只有一个在组头的I帧；第二种说法是针对h264的新特。
结论：
每个GOP一定是以一个I帧开始的，但是却不一定指代的是两个I帧之间的距离。因为一个GOP内可能包含几个I帧，只有第一个I帧（也就是第一帧）才是关键帧。
https://blog.csdn.net/qq_32245927/article/details/80029949
GOP：一段时间内图像变化不大的图像集我们就可以称之为一个序列，这里的图像又在H264里面称为帧，所以就是一组视频帧，其中第一个我们称为是IDR帧。
帧：一副图像编码后的视频数据也叫做一帧，其中有I帧、B帧、P帧，前文多次提到，不再赘述；
片：一帧图像又可以划分为很多片，由一个片或者多个片组成；
宏块：视频编码的最小处理单元，承载了视频的具体YUV信息，一片由一个或者多个宏块组成；
https://blog.csdn.net/sunshineywz/article/details/108089528

IDR(instantaneous decoding refresh)即时解码刷新
目的：【为了解码的重同步】
当解码器解码到 IDR 图像时【相当于向解码器发送了一个清理reference buffer的信号】，立即将参考帧队列清空【驱动器参数块DPB(decord picture buff 参考帧列表)→导致PPS、SPS参数更新】
，将已解码的数据全部输出或抛弃，重新查找参数集，开始一个新的序列。
如果前一个序列出现重大错误，在这里可以获得重新同步的机会。
//为了取得更高的图像质量，在一个视频文件中有好多个IDR帧
IDR frame：I和IDR帧都使用帧内预测，在编码解码中为了方便，首个I帧要和其他I帧区别开，把第一个I帧叫IDR，这样方便控制编码和解码流程，所以IDR帧一定是I帧，但I帧不一定是IDR帧；
IDR帧的作用是立刻刷新,使错误不致传播,从IDR帧开始算新的序列开始编码。I帧有被跨帧参考的可能,IDR不会。

i帧 i frame, intra picture 非参考帧/独立帧  帧内编码帧👈帧内压缩
	通常是GOP的第一个帧(即IDR)，也可能在一个序列中间插入I帧
	一般每秒至少一个I帧
	I帧丢失→随后的P帧也就无法解析	
使用 ffmpeg 获取 I 帧：
指令为：ffmpeg -i wkll.mp4 -ss 00:00:00 -t 1 -r 1 -q:v 2 -f image2 pic-%03d.jpeg

PTS(Presentation Time Stamp)和DTS(Decode Time Stamp): PTS就说这个帧什么时候会放在显示器上;DTS就是说这个帧什么时候被放在编码器去解。
那么如果全是I帧和P帧，PTS和DTS都是单调递增的，那么如果我们有B帧，会出现什么情况?因为大家都知道，对于B帧来讲，它会引用前面的帧和后面的帧。
我们看这个例子，就是当B帧进来的时候，因为它要引用后面的P帧，也要引用前面的I帧，可以看到DTS的顺序，一三四二，然后PTS顺序，一二三四。
B帧它会根据它编码时候的特性，它会自动的把它的DTS时间戳往后挪，把它引用的帧先放到前面去，等它引用的帧解完了，数据解完了之后，才会把B帧去解，
否则的话，我先把B帧放进去，它引用后面的P帧，P帧的数据还没有，B帧解不出来。
所以说这点上给大家讲一下，B帧，P帧，I帧它们整个放在一个视频流里面，它的解码顺序和编码顺序，当然后续的话，我们可能会根据这个有一些开放性的思考。


camera YUV 数据格式知识：
https://blog.csdn.net/whyqcwy/article/details/101546043

ffmpeg解码H264
android下使用ffmpeg解码函数播放H264裸流
https://blog.csdn.net/starhosea/article/details/73128120
ffmpeg解码H264为yuv，并转换成RGB显示到surface上

雷神 csdn ffmpeg
常见视频编码格式：(RAW-->YUV420.YUV422.YUV444-->)H264、H265
常见音频编码格式：(PCM-->)AAC、MP3、G711
常见音视频封装格式：MP4、flv、avi、mov
常见流媒体协议：RTP、RTSP、RTMP、HLS
https://www.jianshu.com/p/558855026e95
https://blog.csdn.net/leixiaohua1020/article/details/15811977
http://blog.csdn.net/leixiaohua1020
https://www.zhihu.com/people/leixiaohua1020
http://jhdr.xhby.net/content/201608/03/c213550.html
1.MediaInfo
https://github.com/MediaArea/MediaInfo
https://MediaArea.net/MediaInfo
2.exiftool
https://github.com/exiftool/exiftool
https://exiftool.org/
3.potplayer
http://potplayer.tv/
https://potplayer.daum.net
4.ffmpeg
OpenGL：billboard算法、perlin噪声技术
OpenGL 三维图形学、图形学基本原理.图形图像处理
Vulkan
OpenCV
OpenAI
FFmepg雷神csdn博客、VLC、Vitamo、ijkplayer
MediaCodec、MediaMuxer、MediaExtractor

MediaCodec：
https://blog.csdn.net/u012521570/article/details/78783294

android 如何保存opengl美颜过的视频
https://blog.csdn.net/u010029439/article/details/89494475

h265.HEVC：
https://blog.csdn.net/NB_vol_1/article/details/51143186

数据源、surface、surfaceTexture(onFrameAvailable(...))、GlSurfaceView(onDrawFrame(GL10 gl))、TextureView各层的回调
数据源-->Surface-->SurfaceTexture-->GlSurfaceView(textureID)、TextureView
1.LocalVideo(ijkPlayer.setDataSource())
2.UsbDevice外设(MediaCodec)：
3.Camera



